"""
Azure OpenAI LLM Client
Azure OpenAI APIë¥¼ ì‚¬ìš©í•œ LLM í†µì‹  í´ë¼ì´ì–¸íŠ¸
"""

import os
import json
import asyncio
from typing import Dict, Any, Optional, List
from openai import AsyncAzureOpenAI
from loguru import logger
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class AzureLLMClient:
    """Azure OpenAI API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        self.api_key = os.getenv("AZURE_OPENAI_API_KEY")
        self.deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
        self.api_version = os.getenv("AZURE_OPENAI_API_VERSION")
        
        if not all([self.endpoint, self.api_key, self.deployment_name, self.api_version]):
            raise ValueError("Azure OpenAI í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        self.client = AsyncAzureOpenAI(
            azure_endpoint=self.endpoint,
            api_key=self.api_key,
            api_version=self.api_version
        )
        
        logger.info("Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]] = None,
        system_prompt: str = None,
        user_prompt: str = None,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        response_format: Optional[Dict[str, str]] = None
    ) -> str:
        """
        Azure OpenAI Chat Completion API í˜¸ì¶œ
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "system", "content": "..."}, ...] (ì„ íƒì‚¬í•­)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            temperature: ì‘ë‹µì˜ ì°½ì˜ì„± (0.0-1.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            response_format: ì‘ë‹µ í˜•ì‹ ì§€ì • (ì˜ˆ: {"type": "json_object"})
        
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            # ë©”ì‹œì§€ êµ¬ì„± ë°©ì‹ ê²°ì •
            if messages is not None:
                # ê¸°ì¡´ ë°©ì‹: messages ì§ì ‘ ì „ë‹¬
                final_messages = messages
            elif system_prompt or user_prompt:
                # ìƒˆ ë°©ì‹: system_prompt, user_prompt ë¶„ë¦¬ ì „ë‹¬
                final_messages = []
                if system_prompt:
                    final_messages.append({"role": "system", "content": system_prompt})
                if user_prompt:
                    final_messages.append({"role": "user", "content": user_prompt})
            else:
                raise ValueError("messages ë˜ëŠ” system_prompt/user_prompt ì¤‘ í•˜ë‚˜ëŠ” ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
            
            # === A2A í”„ë¡œí† ì½œ ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹… ===
            logger.info("=" * 80)
            logger.info("ğŸ¤– LLM API í˜¸ì¶œ ì‹œì‘")
            logger.info("=" * 80)
            
            # ìš”ì²­ íŒŒë¼ë¯¸í„° ë¡œê¹…
            logger.info(f"ğŸ“‹ ìš”ì²­ íŒŒë¼ë¯¸í„°:")
            logger.info(f"   - Model: {self.deployment_name}")
            logger.info(f"   - Temperature: {temperature}")
            logger.info(f"   - Max Tokens: {max_tokens}")
            logger.info(f"   - Response Format: {response_format}")
            logger.info(f"   - Messages Count: {len(final_messages)}")
            
            # ê° ë©”ì‹œì§€ë³„ ìƒì„¸ ë¡œê¹…
            for i, message in enumerate(final_messages, 1):
                role = message.get("role", "unknown")
                content = message.get("content", "")
                
                logger.info(f"\nğŸ“ Message {i} ({role.upper()}):")
                logger.info("-" * 60)
                
                if len(content) > 500:
                    # ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ì•ë’¤ë§Œ ë³´ì—¬ì£¼ê³  ì¤‘ê°„ì€ ìƒëµ
                    logger.info(f"{content[:250]}\n\n... [ì¤‘ê°„ {len(content)-500}ì ìƒëµ] ...\n\n{content[-250:]}")
                else:
                    logger.info(content)
                
                logger.info("-" * 60)
            
            kwargs = {
                "model": self.deployment_name,
                "messages": final_messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            # JSON í˜•ì‹ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
            if response_format:
                kwargs["response_format"] = response_format
            
            logger.info("\nğŸ”„ LLM API í˜¸ì¶œ ì¤‘...")
            response = await self.client.chat.completions.create(**kwargs)
            
            content = response.choices[0].message.content
            
            # ì‘ë‹µ ë¡œê¹…
            logger.info(f"\nâœ… LLM ì‘ë‹µ ìˆ˜ì‹ :")
            logger.info("-" * 60)
            
            if len(content) > 500:
                logger.info(f"{content[:250]}\n\n... [ì¤‘ê°„ {len(content)-500}ì ìƒëµ] ...\n\n{content[-250:]}")
            else:
                logger.info(content)
            
            logger.info("-" * 60)
            
            # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ (ìˆë‹¤ë©´)
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"ğŸ“Š í† í° ì‚¬ìš©ëŸ‰:")
                logger.info(f"   - Prompt Tokens: {response.usage.prompt_tokens}")
                logger.info(f"   - Completion Tokens: {response.usage.completion_tokens}")
                logger.info(f"   - Total Tokens: {response.usage.total_tokens}")
            
            logger.info("=" * 80)
            logger.info("ğŸ¤– LLM API í˜¸ì¶œ ì™„ë£Œ")
            logger.info("=" * 80 + "\n")
            
            return content.strip()
            
        except Exception as e:
            logger.error(f"âŒ Azure OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            logger.error("=" * 80 + "\n")
            raise
    
    async def get_intent_classification(self, user_input: str, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """ì˜ë„ ë¶„ë¥˜ ìš”ì²­"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt.format(user_input=user_input)}
        ]
        
        response = await self.chat_completion(
            messages=messages,
            temperature=0.3,
            max_tokens=500,
            response_format={"type": "json_object"}
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response}")
            return {
                "intent": "chit_chat",
                "confidence": 0.5,
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜"
            }
    
    async def get_entity_extraction(
        self, 
        user_input: str, 
        intent: str, 
        system_prompt: str, 
        user_prompt: str
    ) -> Dict[str, Any]:
        """ì—”í‹°í‹° ì¶”ì¶œ ìš”ì²­"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt.format(user_input=user_input, intent=intent)}
        ]
        
        response = await self.chat_completion(
            messages=messages,
            temperature=0.2,
            max_tokens=500,
            response_format={"type": "json_object"}
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response}")
            return {
                "entities": {},
                "extracted_values": [],
                "confidence": 0.5
            }
    
    async def get_orchestration_decision(
        self,
        user_input: str,
        intent: str,
        entities: Dict[str, Any],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, Any]:
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê²°ì • ìš”ì²­"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt.format(
                user_input=user_input,
                intent=intent,
                entities=json.dumps(entities, ensure_ascii=False)
            )}
        ]
        
        response = await self.chat_completion(
            messages=messages,
            temperature=0.3,
            max_tokens=400,
            response_format={"type": "json_object"}
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response}")
            return {
                "routing_decision": "direct_handle",
                "target_agent": None,
                "action_type": "chat",
                "priority": "low",
                "requires_context": False,
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜"
            }
    
    async def get_chitchat_response(
        self,
        user_input: str,
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, Any]:
        """ì¡ë‹´ ì‘ë‹µ ìƒì„±"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt.format(user_input=user_input)}
        ]
        
        response = await self.chat_completion(
            messages=messages,
            temperature=0.7,
            max_tokens=300,
            response_format={"type": "json_object"}
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "suggest_features": [],
                "tone": "helpful"
            }
    
    async def get_service_response(
        self,
        context: Dict[str, Any],
        system_prompt: str,
        user_prompt: str
    ) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ì—ì´ì „íŠ¸ ì‘ë‹µ ìƒì„±"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt.format(**context)}
        ]
        
        response = await self.chat_completion(
            messages=messages,
            temperature=0.5,
            max_tokens=500,
            response_format={"type": "json_object"}
        )
        
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {response}")
            return {
                "response": "ì„œë¹„ìŠ¤ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "success": False
            }


# LLMClient alias for backward compatibility
LLMClient = AzureLLMClient 